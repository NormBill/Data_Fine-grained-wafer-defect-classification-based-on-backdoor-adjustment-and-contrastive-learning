{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHbsugH4fAIq5LsUolwTfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NormBill/Data_Fine-grained-wafer-defect-classification-based-on-backdoor-adjustment-and-contrastive-learning/blob/main/Cross_Market.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing"
      ],
      "metadata": {
        "id": "RIpZxagMJF-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imop-m3WmXzd",
        "outputId": "7f8f67d1-71d3-489c-ef8e-a2ae8529a398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/ca/Electronics/ratings_ca_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/ca/Electronics/reviews_ca_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/de/Electronics/ratings_de_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/de/Electronics/reviews_de_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/fr/Electronics/ratings_fr_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/fr/Electronics/reviews_fr_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/in/Electronics/ratings_in_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/in/Electronics/reviews_in_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/jp/Electronics/ratings_jp_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/jp/Electronics/reviews_jp_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/mx/Electronics/ratings_mx_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/mx/Electronics/reviews_mx_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/uk/Electronics/ratings_uk_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/uk/Electronics/reviews_uk_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/us/Electronics/ratings_us_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/us/Electronics/reviews_us_Electronics.json.gz\n"
          ]
        }
      ],
      "source": [
        "# import dataset XMarket\n",
        "import os\n",
        "import urllib.request as urlreq\n",
        "\n",
        "tgt_markets = ['ca', 'de', 'fr', 'in', 'jp', 'mx', 'uk', 'us']\n",
        "tgt_cat = 'Electronics'\n",
        "\n",
        "fix_url = 'https://ciir.cs.umass.edu/downloads/XMarket/FULL/'\n",
        "orig_data_dl = 'DATA2/orig_data'\n",
        "proc_data_out = 'DATA2/proc_data'\n",
        "if not os.path.exists(orig_data_dl):\n",
        "    os.makedirs(orig_data_dl)\n",
        "if not os.path.exists(proc_data_out):\n",
        "    os.makedirs(proc_data_out)\n",
        "    \n",
        "for tgt_market in tgt_markets:\n",
        "    cur_url = f'{fix_url}{tgt_market}/{tgt_cat}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "    urlreq.urlretrieve(cur_url, f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz')\n",
        "    print(f'Done: {cur_url}')\n",
        "    cur_url_review = f'{fix_url}{tgt_market}/{tgt_cat}/reviews_{tgt_market}_{tgt_cat}.json.gz'\n",
        "    urlreq.urlretrieve(cur_url_review, f'{orig_data_dl}/reviews_{tgt_market}_{tgt_cat}.json.gz')\n",
        "    print(f'Done: {cur_url_review}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "# set thr for K-core data cleaning. We use 5core, +1 valid and +1 test-> so at this point we filter 7core\n",
        "# set thresholds for item interaction \n",
        "user_thr = 7\n",
        "item_thr = 7\n",
        "\n",
        "# one can iterate a few times, we only perform one time filter\n",
        "def get_kcore(ratings_all, user_thr, item_thr, repeat=1):\n",
        "    for i in range(repeat):\n",
        "        ratings_all.reset_index(drop=True, inplace=True)\n",
        "        ratings_all = ratings_all.loc[ratings_all.groupby(\"itemId\").filter(lambda x: len(x) >= item_thr).index]\n",
        "        ratings_all.reset_index(drop=True, inplace=True)\n",
        "        ratings_all = ratings_all.loc[ratings_all.groupby(\"userId\").filter(lambda x: len(x) >= user_thr).index]\n",
        "        ratings_all.reset_index(drop=True, inplace=True)\n",
        "    return ratings_all\n",
        "\n",
        "def rating_stats(ratings_all):\n",
        "    n_rating = ratings_all.shape[0]\n",
        "    n_user = len(set(ratings_all['userId'].unique()))\n",
        "    n_item = len(set(ratings_all['itemId'].unique()))\n",
        "    \n",
        "    if (n_user*n_item)!=0:\n",
        "        density = round((n_rating/(n_user*n_item) )*100, 5)\n",
        "    else:\n",
        "        density = 0\n",
        "        \n",
        "    return { '#users': n_user,\n",
        "        '#items': n_item,\n",
        "        '#rates': n_rating,\n",
        "        'dens\\%': density,\n",
        "        }"
      ],
      "metadata": {
        "id": "pmKJD4EHm0tk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Clean US (base) —— rating \n",
        "# import pandas as pd\n",
        "\n",
        "# tgt_market = 'us'\n",
        "# us_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "\n",
        "# us_df = pd.read_csv(us_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "# us_df_7core = get_kcore(us_df, user_thr=user_thr, item_thr=item_thr)\n",
        "\n",
        "# us_user_thr = 10\n",
        "# us_item_thr = 25\n",
        "# us_df_10core = get_kcore(us_df, user_thr=us_user_thr, item_thr=us_item_thr)\n",
        "\n",
        "# # write us data \n",
        "# us_df_7core.to_csv(f'{proc_data_out}/{tgt_market}_5core.txt', index=False, sep=' ')\n",
        "# us_df_10core.to_csv(f'{proc_data_out}/{tgt_market}_10core.txt', index=False, sep=' ')\n",
        "\n",
        "# print('US Market data stats:')\n",
        "# print(rating_stats(us_df))\n",
        "# print(rating_stats(us_df_7core))\n",
        "# print(rating_stats(us_df_10core))\n",
        "import pandas as pd\n",
        "\n",
        "tgt_market = 'us'\n",
        "us_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "\n",
        "us_df = pd.read_csv(us_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "\n",
        "# Random sample 0.5% of the data\n",
        "us_df_sample = us_df.sample(frac=0.005)\n",
        "\n",
        "# write us sampled data \n",
        "us_df_sample.to_csv(f'{proc_data_out}/{tgt_market}_sample.txt', index=False, sep=' ')\n",
        "\n",
        "print('US Market data stats:')\n",
        "print(rating_stats(us_df))\n",
        "print(rating_stats(us_df_sample))"
      ],
      "metadata": {
        "id": "mSgNt8eDpKs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb601ed-3904-4aae-818e-5f3bf5fdc334"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "US Market data stats:\n",
            "{'#users': 2784128, '#items': 35943, '#rates': 4169476, 'dens\\\\%': 0.00417}\n",
            "{'#users': 20732, '#items': 12939, '#rates': 20847, 'dens\\\\%': 0.00777}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Clean target —— rating\n",
        "# us_items_set = set(us_df_10core['itemId'].unique())\n",
        "\n",
        "# for tgt_market in tgt_markets:\n",
        "#     if tgt_market=='us':\n",
        "#         continue\n",
        "#     #read market ratings\n",
        "#     cur_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "#     cur_df = pd.read_csv(cur_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "    \n",
        "#     # item exist for us \n",
        "#     cur_df = cur_df.loc[cur_df['itemId'].isin( us_items_set )] \n",
        "#     cur_df_7core = get_kcore(cur_df, user_thr=user_thr, item_thr=item_thr)\n",
        "#     cur_df_7core.to_csv(f'{proc_data_out}/{tgt_market}_5core.txt', index=False, sep=' ')\n",
        "#     print(f'\\n-- {tgt_market} stats: ')\n",
        "#     print(rating_stats(cur_df_7core))\n",
        "us_items_set = set(us_df_sample['itemId'].unique())\n",
        "\n",
        "for tgt_market in tgt_markets:\n",
        "    if tgt_market=='us':\n",
        "        continue\n",
        "    # read market ratings\n",
        "    cur_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "    cur_df = pd.read_csv(cur_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "    \n",
        "    # items that exist in US\n",
        "    cur_df = cur_df.loc[cur_df['itemId'].isin( us_items_set )] \n",
        "    \n",
        "    # Random sample 0.5% of the data\n",
        "    cur_df_sample = cur_df.sample(frac=0.005)\n",
        "    \n",
        "    cur_df_sample.to_csv(f'{proc_data_out}/{tgt_market}_sample.txt', index=False, sep=' ')\n",
        "    print(f'\\n-- {tgt_market} stats: ')\n",
        "    print(rating_stats(cur_df_sample))"
      ],
      "metadata": {
        "id": "hE8XvPWUpS1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b4bd31-aee9-4b75-e22f-ddf7c7d9a26e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- ca stats: \n",
            "{'#users': 1730, '#items': 982, '#rates': 1734, 'dens\\\\%': 0.10207}\n",
            "\n",
            "-- de stats: \n",
            "{'#users': 1267, '#items': 523, '#rates': 1269, 'dens\\\\%': 0.19151}\n",
            "\n",
            "-- fr stats: \n",
            "{'#users': 921, '#items': 392, '#rates': 922, 'dens\\\\%': 0.25538}\n",
            "\n",
            "-- in stats: \n",
            "{'#users': 661, '#items': 213, '#rates': 662, 'dens\\\\%': 0.47019}\n",
            "\n",
            "-- jp stats: \n",
            "{'#users': 418, '#items': 223, '#rates': 418, 'dens\\\\%': 0.44843}\n",
            "\n",
            "-- mx stats: \n",
            "{'#users': 446, '#items': 268, '#rates': 451, 'dens\\\\%': 0.37732}\n",
            "\n",
            "-- uk stats: \n",
            "{'#users': 1823, '#items': 764, '#rates': 1831, 'dens\\\\%': 0.13146}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Clean US (base) —— review\n",
        "import numpy as np  \n",
        "from itertools import islice\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "tgt_market = 'us'\n",
        "us_reviews_file = f'{orig_data_dl}/reviews_{tgt_market}_{tgt_cat}.json.gz'\n",
        "\n",
        "# read market reviews\n",
        "sample_ratio = 0.03  \n",
        "\n",
        "# unzip\n",
        "with gzip.open(us_reviews_file, 'rt') as f:\n",
        "  # read json by line\n",
        "  lines = list(islice(f, int(sample_ratio * sum(1 for _ in f))))\n",
        "\n",
        "cur = [json.loads(line) for line in lines]\n",
        "\n",
        "\n",
        "us_review_sum = []\n",
        "for row in cur:\n",
        "    for dict in row[0]:\n",
        "      us_review_sum.append(dict[\"summary\"])\n",
        "\n",
        "us_review_sum = pd.DataFrame(us_review_sum)    \n",
        "us_review_sum.to_csv(f'{proc_data_out}/{tgt_market}_reviews.txt', index=False, sep=' ')\n"
      ],
      "metadata": {
        "id": "1wnsP9kIJAyv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Clean target —— review \n",
        "from itertools import islice\n",
        "\n",
        "us_items_set = pd.DataFrame(us_items_set)\n",
        "\n",
        "for tgt_market in tgt_markets:\n",
        "    if tgt_market=='us':\n",
        "        continue\n",
        "    # read market reviews\n",
        "    cur_reviews_file = f'{orig_data_dl}/reviews_{tgt_market}_{tgt_cat}.json.gz'\n",
        "    sample_ratio = 0.03  \n",
        "\n",
        "    # unzip\n",
        "    with gzip.open(cur_reviews_file, 'rt') as f:\n",
        "        # read json by line\n",
        "        lines = list(islice(f, int(sample_ratio * sum(1 for _ in f))))\n",
        "\n",
        "    cur = [json.loads(line) for line in lines]\n",
        "    \n",
        "    for row in cur:\n",
        "      row[0] = [dict for dict in row[0] if dict[\"asin\"] in us_items_set]\n",
        "\n",
        "\n",
        "    cur_review_sum = []\n",
        "    for row in cur:\n",
        "        for dict in row:\n",
        "          cur_review_sum.append(dict[\"summary\"])\n",
        "\n",
        "    cur_review_sum = pd.DataFrame(cur_review_sum)\n",
        "    \n",
        "    cur_review_sum.to_csv(f'{proc_data_out}/{tgt_market}_reviews.txt', index=False, sep=' ')"
      ],
      "metadata": {
        "id": "5hb2zNmRJA4K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Construction"
      ],
      "metadata": {
        "id": "goOfH-rGJOx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predefinition"
      ],
      "metadata": {
        "id": "sWDbBab9OaXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install args\n",
        "!pip install bpemb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqjbowIiM9OG",
        "outputId": "dec4a0fc-3b64-4925-9584-f0a035b79024"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting args\n",
            "  Downloading args-0.1.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: args\n",
            "  Building wheel for args (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for args: filename=args-0.1.0-py3-none-any.whl size=3320 sha256=e67770df62fee54bf790afed990b538332752918f85bc5eff4829f7f3a93d403\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/d7/bc/7b88d8405d97070a1a62712fd639ea0ad8d14b3dd74075cca6\n",
            "Successfully built args\n",
            "Installing collected packages: args\n",
            "Successfully installed args-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.27.1)\n",
            "Collecting sentencepiece (from bpemb)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.4 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "data_info_n2m = {\n",
        "\t\"mind_0_0shot\": {\n",
        "\t\t'user_n': 1000,\n",
        "\t\t'news_n': 5000,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"mind_200_4shot\":{\n",
        "\t\t'user_n': 2500,\n",
        "\t\t'news_n': 8000,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"adressa\": {\n",
        "\t\t'user_n' : 10000,\n",
        "\t\t'news_n' : 2097,\n",
        "\t\t'max_title_token': 30,\n",
        "\t}\n",
        "}\n",
        "\n",
        "data_info_m2n = {\n",
        "\t\"mind_0_0shot\": {\n",
        "\t\t'user_n': 1000,\n",
        "\t\t'news_n': 708,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"mind_200_2shot\": {\n",
        "\t\t'user_n': 1000,\n",
        "\t\t'news_n': 646,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"mind_200_4shot\":{\n",
        "\t\t'user_n': 1000,\n",
        "\t\t'news_n': 708,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"mind_200_6shot\": {\n",
        "\t\t'user_n': 1000,\n",
        "\t\t'news_n': 692,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"mind_5500_10shot\": {\n",
        "\t\t'user_n': 13500,\n",
        "\t\t'news_n': 1862,\n",
        "\t\t'max_title_token': 30,\n",
        "\t},\n",
        "\n",
        "\t\"adressa\": {\n",
        "\t\t'user_n' : 10000,\n",
        "\t\t'news_n' : 31099,\n",
        "\t\t'max_title_token': 30,\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "pretrained_embed = 300\n",
        "deepwalk_embed = 300\n",
        "embed_d = 300\n",
        "\n",
        "def read_args(db='mind', lr=0.0002):\n",
        "\n",
        "\tparser = argparse.ArgumentParser()\n",
        "\tparser.add_argument('--few_shot', type=str, default='_0_0shot')  # ''/'_100'/'_500'/'_1000'\n",
        "\tparser.add_argument('--db', type = str, default = db,\n",
        "\t\t\t\t   help = 'node net dimension')\n",
        "\n",
        "\tparser.add_argument('--embed_d', type = int, default = embed_d,\n",
        "\t\t\t\t   help = 'embedding dimension')\n",
        "\n",
        "\tparser.add_argument('--lr', type = int, default = lr,\n",
        "\t\t\t\t   help = 'learning rate')\n",
        "\n",
        "\tparser.add_argument('--batch_s', type = int, default = 20000,\n",
        "\t\t\t\t   help = 'batch size')\n",
        "\n",
        "\tif db == 'mind':\n",
        "\t\tmini_batch_s = 80\n",
        "\telse:\n",
        "\t\tmini_batch_s = 80\n",
        "\n",
        "\tparser.add_argument('--mini_batch_s', type = int, default = mini_batch_s,\n",
        "\t\t\t\t   help = 'mini batch size')\n",
        "\n",
        "\tparser.add_argument('--train_iter_n', type = int, default = 10,\n",
        "\t\t\t\t   help = 'max number of training iteration')\n",
        "\tparser.add_argument(\"--random_seed\", default = 42, type = int)\n",
        "\n",
        "\tparser.add_argument('--save_model_freq', type = float, default = 1,\n",
        "\t\t\t\t   help = 'number of iterations to save model')\n",
        "\tparser.add_argument(\"--cuda\", default = 2, type = int)\n",
        "\tparser.add_argument(\"--checkpoint\", default = '', type=str)\n",
        "\tparser.add_argument(\"--npratio\", default=4, type=int)\n",
        "\n",
        "\t\"\"\"\n",
        "\tSome other parameters needed to be test\n",
        "\t\"\"\"\n",
        "\tparser.add_argument(\"--save_emb\", default=0, type=int)\n",
        "\n",
        "\t# ablation studies for different modules\n",
        "\tparser.add_argument(\"--use_PLM\", default=1, type=int)\n",
        "\tparser.add_argument(\"--use_KG\", default=0, type=int)\n",
        "\t# few-shot setting\n",
        "\tparser.add_argument(\"--few_shot_method\", default=2, type=int)  # 0-only mind 1-mind+adressa 2-ours\n",
        "\t# other domains\n",
        "\tparser.add_argument(\"--range\", default=\"Model/engTonor\", type=str) # Model/data  Model/engTonor\n",
        "\n",
        "\tparser.add_argument(\"--align_mode\", default=\"no_freeze\", type=str)\n",
        "\n",
        "\tparser.add_argument(\"--loss_weight\", default=0.2, type=float)\n",
        "\tparser.add_argument(\"--loss_weight_align\", default=1, type=float)\n",
        "\tparser.add_argument(\"--news_cls_iter\", default=1, type=int)\n",
        "\t# target domain sim\n",
        "\tparser.add_argument(\"--target_domain_sim\", default=0.6, type=float)\n",
        "\n",
        "\t# top-n plm news\n",
        "\tparser.add_argument(\"--topn\", default=1, type=int)\n",
        "\n",
        "\n",
        "\targs = parser.parse_args()\n",
        "\n",
        "\tif db == 'mind':\n",
        "\t\tdata_key = db + args.few_shot\n",
        "\telse:\n",
        "\t\tdata_key = db\n",
        "\n",
        "\trange = args.range\n",
        "\tif range == 'Model/data' or range == 'Model/new_data' or range != 'Model/engTonor':\n",
        "\t\tdata_info = data_info_n2m\n",
        "\t\tif data_key in data_info:\n",
        "\t\t\targs.A_n = data_info[data_key]['user_n']\n",
        "\t\t\targs.P_n = data_info[data_key]['news_n']\n",
        "\t\t\targs.max_title_token = data_info[data_key]['max_title_token']\n",
        "\t\telse:\n",
        "\t\t\targs.A_n = data_info[\"mind_200_4shot\"]['user_n']\n",
        "\t\t\targs.P_n = data_info[\"mind_200_4shot\"]['news_n']\n",
        "\t\t\targs.max_title_token = data_info[\"mind_200_4shot\"]['max_title_token']\n",
        "\telse:\n",
        "\t\tdata_info = data_info_m2n\n",
        "\n",
        "\t\tif data_key in data_info:\n",
        "\t\t\targs.A_n = data_info[data_key]['user_n']\n",
        "\t\t\targs.P_n = data_info[data_key]['news_n']\n",
        "\t\t\targs.max_title_token = data_info[data_key]['max_title_token']\n",
        "\t\telse:\n",
        "\t\t\targs.A_n = data_info[\"mind_5500_10shot\"]['user_n']\n",
        "\t\t\targs.P_n = data_info[\"mind_5500_10shot\"]['news_n']\n",
        "\t\t\targs.max_title_token = data_info[\"mind_5500_10shot\"]['max_title_token']\n",
        "\n",
        "\targs.data_path = '../{}/{}/'.format(range, data_key)\n",
        "\targs.model_path = './model_save/{}/'.format(data_key)\n",
        "\treturn args"
      ],
      "metadata": {
        "id": "8WbFfUAjORCH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(torch.nn.Module):\n",
        "    def __init__(self, in_dim=100, v_size=200):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.v_size = v_size\n",
        "        # self.v = torch.nn.Parameter(torch.rand(self.v_size))\n",
        "        self.proj = nn.Sequential(nn.Linear(self.in_dim, self.v_size), nn.Tanh())\n",
        "        self.proj_v = nn.Linear(self.v_size, 1)\n",
        "\n",
        "    def forward(self, context, mask=None):\n",
        "        \"\"\"Additive Attention\n",
        "\n",
        "        Args:\n",
        "            context (tensor): [B, seq_len, in_dim]\n",
        "\n",
        "        Returns:\n",
        "            outputs, weights: [B, seq_len, out_dim], [B, seq_len]\n",
        "        \"\"\"\n",
        "        # weights = self.proj(context) @ self.v\n",
        "        weights = self.proj_v(self.proj(context)).squeeze(-1)    # B*seq\n",
        "\n",
        "        if mask is not None:\n",
        "            weights = weights.masked_fill(mask, float(\"-inf\"))\n",
        "        weights = torch.softmax(weights, dim=-1) # [B, seq_len]\n",
        "        return torch.bmm(weights.unsqueeze(1), context).squeeze(1), weights # [B, 1, seq_len], [B, seq_len, dim]\n"
      ],
      "metadata": {
        "id": "Wib-FdFwNITS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "import numpy as np\n",
        "import math\n",
        "from bpemb import BPEmb\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import pandas as pd\n",
        "import pdb\n",
        "class PredLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Prediction layer (cross_entropy or adaptive_softmax).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_words, emb_dim, asm=False):\n",
        "        super().__init__()\n",
        "        self.n_words = n_words\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # if asm is False:\n",
        "        self.proj = nn.Linear(self.emb_dim, self.n_words, bias=True)\n",
        "        # else:\n",
        "        #     self.proj = nn.AdaptiveLogSoftmaxWithLoss(\n",
        "        #         in_features=self.emb_dim,\n",
        "        #         n_classes=self.n_words,\n",
        "        #         cutoffs=params.asm_cutoffs,\n",
        "        #         div_value=params.asm_div_value,\n",
        "        #         head_bias=True,  # default is False\n",
        "        #     )\n",
        "\n",
        "    def forward(self, x, y, get_scores=False):\n",
        "        \"\"\"\n",
        "        Compute the loss, and optionally the scores.\n",
        "        \"\"\"\n",
        "        scores = self.proj(x).view(-1, self.n_words)\n",
        "        loss = F.cross_entropy(scores, y, reduction='mean')\n",
        "        return scores, loss"
      ],
      "metadata": {
        "id": "amZA4KGXMvnE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Review Embedding\n"
      ],
      "metadata": {
        "id": "ghPUVl5aQQYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb = BPEmb(lang=\"en\", vs=1000, dim=300)\n",
        "\n",
        "file_path = \"/content/DATA2/proc_data/us_reviews.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    sentences = [line.strip() for line in file]\n",
        "\n",
        "# Embed the sentences\n",
        "embeddings = []\n",
        "for sentence in sentences:\n",
        "    encoded_sentence = bpemb.encode(sentence)\n",
        "    embedding = np.mean(bpemb.vectors[encoded_sentence], axis=0)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Convert the embeddings list to a NumPy array\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Save the embeddings to a text file\n",
        "output_file = \"us_embeddings.txt\"\n",
        "np.savetxt(output_file, embeddings)\n",
        "\n",
        "print(\"Embeddings have been saved to the file:\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrV7eXouQU0_",
        "outputId": "e4d6df1a-6087-490c-da0d-ff60198e7904"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have been saved to the file: us_embeddings.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb = BPEmb(lang=\"en\", vs=1000, dim=300)\n",
        "\n",
        "for tgt_market in tgt_markets:\n",
        "    file_path = f'/content/DATA2/proc_data/{tgt_market}_reviews.txt'\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = [line.strip() for line in file]\n",
        "\n",
        "    # Embed the sentences\n",
        "    embeddings = []\n",
        "    for sentence in sentences:\n",
        "      embedding = bpemb.embed(sentence)\n",
        "      mean_embedding = np.mean(embedding, axis=0)\n",
        "      embeddings.append(mean_embedding)\n",
        "\n",
        "    # Convert the embeddings list to a NumPy array\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Save the embeddings to a text file\n",
        "    output_file = f'{tgt_market}_embeddings.txt'\n",
        "    np.savetxt(output_file, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXe_T48YWDfH",
        "outputId": "621a8012-95a2-4111-8bbc-fdd9acd40fe2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret = um.true_divide(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HetGNN Model"
      ],
      "metadata": {
        "id": "T55f9aO6eJRs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRLLO-1WQArl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}